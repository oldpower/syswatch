import requests
import json
import time
from multiprocessing import Pool
from openai import OpenAI

def unstreamfunc(modelname):
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": modelname,
            "prompt": "ç®€å•è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—.",
            "stream": False
        }
    )
    # print(response.json()["response"])
    timeconsuming(response.json())

def stream_completion(model_name):
    """
    è°ƒç”¨ vLLM çš„æµå¼ Completion æ¥å£ï¼Œå¹¶ç»Ÿè®¡ token ç”Ÿæˆé€Ÿåº¦ã€‚
    
    å‚æ•°:
        model_name (str): æ¨¡å‹åç§°
        prompt_text (str): è¾“å…¥çš„æç¤ºè¯
        max_tokens (int): æœ€å¤šç”Ÿæˆ token æ•°
    """
    client = OpenAI(base_url="http://192.168.103.203:11435/v1", api_key="token-abc123")
    start_time = time.time()
    total_tokens = 0
    prompt_text = "è¯¦ç»†ä»‹ç»ä¸€ä¸‹é‡å­è®¡ç®—."
    stream = client.completions.create(
        model=model_name,
        prompt=prompt_text,
        stream=True,
        max_tokens=2048
    )

    try:
        for chunk in stream:
            text = chunk.choices[0].text
            tokens = len(text.split())  # ç²—ç•¥ä¼°è®¡ token æ•°ï¼ˆå¯æ›¿æ¢ä¸º tokenizerï¼‰
            total_tokens += tokens

            elapsed_time = time.time() - start_time
            speed = total_tokens / elapsed_time if elapsed_time > 0 else 0
            # print(text, end="", flush=True)
            # print(f"\r[å®æ—¶é€Ÿåº¦: {speed:.2f} Tokens/ç§’]", end="")
            # time.sleep(0.1)

        total_time = time.time() - start_time
        avg_speed = total_tokens / total_time if total_time > 0 else 0
        # print("\n\n=== ç”Ÿæˆç»“æŸ ===")
        # print(f"æ€»ç”Ÿæˆ Token æ•°: {total_tokens}")
        # print(f"æ€»è€—æ—¶: {total_time:.2f} ç§’")
        print(f"å¹³å‡é€Ÿåº¦: {avg_speed:.2f} Tokens/ç§’")

    except Exception as e:
        print("\n[é”™è¯¯] æµå¼è¯»å–è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸:", str(e))


def timeconsuming(response):
    # æå–å…³é”®å­—æ®µ
    eval_count = response["eval_count"]
    eval_duration_ns = response["eval_duration"]
    prompt_eval_count = response["prompt_eval_count"]
    prompt_duration_ns = response["prompt_eval_duration"]

    # è®¡ç®—é€Ÿåº¦
    gen_speed = eval_count / (eval_duration_ns * 1e-9)
    prompt_speed = prompt_eval_count / (prompt_duration_ns * 1e-9)

    print(f"ğŸ›¸ç”Ÿæˆå¹³å‡é€Ÿåº¦: {gen_speed:.2f} Tokens/ç§’")
    # print(f"Prompt å¤„ç†é€Ÿåº¦: {prompt_speed:.2f} Tokens/ç§’")

# streamfunc()

if __name__ == "__main__":
    modelnames = ["Qwen/Qwen3-1.7B","Qwen/Qwen3-1.7B-FP8",
                  "Qwen/Qwen3-4B","Qwen/Qwen3-4B-AWQ",
                  "Qwen/Qwen3-8B","Qwen/Qwen3-8B-AWQ",
                  "Qwen/Qwen3-14B","Qwen/Qwen3-14B-AWQ",
                  "qwen3:4b"  ,"qwen3:4b-fp16"  ,
                  "qwen3:8b",
                  "qwen3:14b"]

    modelname = modelnames[5]
    # stream_completion(modelname)
    print(f"ğŸš€modelname is {modelname}")
    # print("ğŸ‘‡éæµ:")
    # unstreamfunc(modelname=modelname)
    # print()
    # print("ğŸ‘‡æµå¼:")
    # streamfunc(modelname=modelname)

    num_processes = 4 
    print(f"num_processes is {num_processes}")
    with Pool(processes=num_processes) as pool:
        # ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ååˆ›å»ºå‚æ•°åˆ—è¡¨
        pool.map(stream_completion, [modelname]*num_processes)
